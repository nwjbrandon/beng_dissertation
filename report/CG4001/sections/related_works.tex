\section{Related Works}
\noindent
3D pose estimation is the problem of estimating the 3D joint locations from an image frame. Earlier methods attempt to predict the 3D poses from a volumetric space; however, the accuracy  of the poses is limited by the output resolution \cite{poseestimationreview}. Current methods treat the task of estimating the 3D poses as a regression problem that is simpler and requires less computation \cite{poseestimationreview}. The pose estimation models can be trained to learn the joint locations from the image directly or can be trained using a separate lightweight network to learn the 3D poses from 2D poses \cite{baseline, poseestimationreview}. 

\noindent
The common issue in estimating 3D poses is the ambiguity in the mapping from 2D poses to 3D poses \cite{poseestimationreview}. There are methods that use temporal information of poses and multi-view frames to address this ambiguity \cite{poseestimationreview}. Since the image provides rich information about the 3D pose, several methods attempt to encode the image features to provide an embedding for the model to learn the 3D poses \cite{blazepose, handgcn, olha}. The encoded features were supervised by the ground truths of the 2D poses.

\noindent
The encoder-decoder architecture is a common architecture used in pose estimation models to encode information in the image \cite{olha}. Pose estimation models use this architecture to return 2D heat maps of the same size as the input images \cite{olha}. The heat maps indicate where the keypoints of the 2D poses are in the image. The encoder extracts information from the image and represent as an embedding \cite{blazepose}. The decoder uses the embedding to localise the keypoint in the image. Skip connections are also used to combine the high and low level information to obtain more precise decoding \cite{olha, blazepose}. This section primarily examines the works contributed by Chernytska, Bazarevsky, and Zhao that use the encoder-decoder architecture and are relevant to this paper \cite{blazepose, handgcn, olha}.

\noindent
Chernytska used a ResNet-based encoder-decoder architecture for 3D hand pose estimation models in 2019 \cite{olha}. The model used ResNet34 as the encoder. In ResNet34, the image passed through 1 convolution layer, followed by 4 residual layers where each layer is made of 3, 4, 6, 3 residual blocks respectively \cite{resnet}. In the decoder, 4 upsampling blocks were used where the input from the previous decoding layer is upsampled and concatenated with the input from the intermediate residual layers of the encoder \cite{olha}. The output from the last upsampling block is upsampled once more to return 2D heat-maps of the same size as the original image \cite{olha}. The peak of 2D heat-maps indicates where the keypoints of the 2D poses are in the image. To predict the keypoints for the 3D poses, the output from the first residual layer of the encoder is concatenated with the heatmaps and passed to a regression module \cite{olha}. The last layer of the regression module is a fully-connected layer of output size 3 \(\times\) the number of joints to estimate the x, y, and z coordinates for each joint \cite{olha}. The model was trained using 2 loss functions - one for the 2D heatmaps and one for the 3D keypoints \cite{olha}.

\noindent
ResNet is commonly used as the feature extractor in many deep learning computer vision tasks. ResNet contains residual blocks which has a stack of few convolution layers \cite{resnet}. Each block fits onto a residual mapping \cite{resnet}. The residual block also has a skip connection where element-wise addition is performed on the input of the block and the output of the last layer of the block \cite{resnet}.  Equation \ref{eq:residual_connection} shows the residual mapping where \(x\) denotes the input, \(F(x)\) denotes the residual mapping, and \(H(x)\) denotes the desired mapping \cite{resnet}. Kaiming mentioned that it is easier for a stack of a few layers to learn and push the error to zero in residual mapping instead of a network learning the desired network due to vanishing gradient \cite{resnet}.
\begin{equation}
F(x) = H(x) - x \label{eq:residual_connection}
\end{equation}
\noindent
Zhao proposed an improved graph convolution module - Semantic Graph Convolution (SemGCN) - for 3D body pose estimation in 2019 \cite{semgcn}. SemGCN represents the body pose as a graph where the nodes are the joints and edges are the bones \cite{semgcn}. The graph convolution operation is shown in Equation \ref{eq:modified_gcn_convolutional_operation_receptive_field} and is an adaption of Graph Convolution Network (GCN) \cite{semgcn}. In GCN, the input \(\mathbf{X}\) is transformed by a learnable matrix \(\mathbf{W}\) and multiplied by a normalized adjacency matrix \(\mathbf{A}\) to learn features from neighbouring nodes in the graph \cite{semgcn}. However, the same learnable matrix \(\mathbf{W}\) is applied across the graph and limits the internal representative of the graph \cite{semgcn}. Zhao introduced a second learnable matrix \(\mathbf{M}\) to encode information for the each sub-graph in GCN \cite{semgcn}. This forms the improved GCN - SemGCN \cite{semgcn}. In Zhao's paper, he proposed two types of SemGCN models. The first model used 4 blocks, each block has 2 SemGCN layers, to take in 2D keypoints and outputs 3D keypoints \cite{semgcn}. The second model used a pre-trained 2D model with  ResNet50 as the encoder \cite{semgcn}. The features from the intermediate residual layers and heatmaps are flatten, concatenated and passed to the SemGCN model to estimate the 3D keypoints \cite{semgcn}. Both SemGCN models are supervised by two loss functions - one for the 3D bone vector and one for the 3D keypoints \cite{semgcn}.
\begin{equation}
\mathbf{X}^{l+1}=\sigma(\mathbf{W}\mathbf{X}^{l}\rho_i(\mathbf{M}\odot\mathbf{A})) \label{eq:modified_gcn_convolutional_operation_receptive_field}
\end{equation}
\noindent
In SemGCN, Zhao also used non-local layers to learn non-local relationship between nodes in the graph \cite{semgcn}. The idea of non-local layers was first introduced by Wang for video classification to increase the receptive field of convolution layers without increasing the computation cost due to a larger kernel size \cite{nonlocal}. The non-local operation is described in Equation \ref{eq:non_local_operation} and \ref{eq:non_local_f_operation} \cite{nonlocal}. The function \(f\) computes the affinity or relative importance between two entities, influencing the information contribution by the entities at difference location in the image or graph \cite{nonlocal}. 
\begin{equation}
\overrightarrow{x_i}^{l+1}=\overrightarrow{x_i}^{l}+\frac{W_x}{K}\sum_{j=1}^{K}f(\overrightarrow{x_i}^{l},\overrightarrow{x_j}^{l}).g(\overrightarrow{x_J}^{l}) \label{eq:non_local_operation}
\end{equation}
\begin{equation}
f(\overrightarrow{x_i}^{l},\overrightarrow{x_j}^{l})=\relu(\mathbf{w}_f [\theta(\overrightarrow{x_i}^{l})||\phi(\overrightarrow{x_j}^{l})]) \label{eq:non_local_f_operation}
\end{equation}
\noindent
Bazarevsky proposed BlazePose as a lightweight 2D body pose estimation model for real-time inference on mobile devices in 2020 \cite{blazepose}. BlazePose is available in the MediaPipe Python package to use out of the box \cite{blazepose}. The model used an encoder-decoder architecture with skip connections at each stage of the network \cite{blazepose}. The exact model architecture is not specified in the paper. The decoder networks consists of 4 upsampling blocks and outputs heat maps and offset maps of size \(\frac{1}{4}\) the size of the original image \cite{blazepose}. The heat maps estimates where the 2D keypoints are located in the image while the offset maps estimates the offset of the joint position at the pixel to overcome the limitation of the heat maps output resolution. The encoder-decoder network is trained and supervised by the heat map loss function and offset map loss function \cite{blazepose}. The decoder is then removed, and the encoder is freezed and stacked with a regression network \cite{blazepose}. The encoder serves as a lightweight embeddings \cite{blazepose}. The encoder and regressor is trained to estimate the 2D keypoints and the visibility \cite{blazepose}. Bazarevsky mentioned that this approach is more scalable and computationally less intensive \cite{blazepose}. The output layer is a fully-connected layer and can be modified to include more attributes for each keypoints \cite{blazepose}. The model is trained on an in-house dataset that is restricted to poses where certain keypoints can confidently annotated, such as the hips and shoulders \cite{blazepose}.
